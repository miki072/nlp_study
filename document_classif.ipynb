{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "#bbcのニュース記事のデータを用いて、英文ニュースのジャンルを\n",
    "#５つのジャンルで分類するモデルをkerasを焼死作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "os.chdir(r\"C:\\Users\\81702\\kagglechallnge\\bbc\\bbc-fulltext (document classification)\\bbc\\business\")\n",
    "pathlist = [r\"C:\\Users\\81702\\kagglechallnge\\bbc\\bbc-fulltext (document classification)\\bbc\\business\", \n",
    "            r\"C:\\Users\\81702\\kagglechallnge\\bbc\\bbc-fulltext (document classification)\\bbc\\entertainment\",\n",
    "           r\"C:\\Users\\81702\\kagglechallnge\\bbc\\bbc-fulltext (document classification)\\bbc\\politics\",\n",
    "            r\"C:\\Users\\81702\\kagglechallnge\\bbc\\bbc-fulltext (document classification)\\bbc\\sport\",\n",
    "           r\"C:\\Users\\81702\\kagglechallnge\\bbc\\bbc-fulltext (document classification)\\bbc\\tech\"]\n",
    "topics = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
    "\n",
    "\n",
    "dirPath = r\"C:\\Users\\81702\\kagglechallnge\\bbc\\bbc-fulltext (document classification)\\bbc\\business\\*\"\n",
    "result = glob.glob(dirPath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "allfiles = []\n",
    "for path in pathlist:\n",
    "    path = path + \"\\*\"\n",
    "    files = glob.glob(path)\n",
    "    allfiles.append(files)\n",
    "    \n",
    "print(len(allfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "alltexts = []\n",
    "for files, topic in zip(allfiles, topics):\n",
    "    texts = []\n",
    "    for file in files:\n",
    "        f = open(file)\n",
    "        data = f.read()\n",
    "        data = [data, topic]\n",
    "        texts.append(data)\n",
    "    alltexts.append(texts)\n",
    "        \n",
    "print(len(alltexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business', 'entertainment', 'politics', 'sport', 'tech']\n"
     ]
    }
   ],
   "source": [
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "business = pd.DataFrame(alltexts[0])\n",
    "entertainment = pd.DataFrame(alltexts[1])\n",
    "politics = pd.DataFrame(alltexts[2])\n",
    "sport = pd.DataFrame(alltexts[3])\n",
    "tech = pd.DataFrame(alltexts[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(alltexts[0])\n",
    "\n",
    "for i in range(1,4):\n",
    "    new = pd.DataFrame(alltexts[i])\n",
    "    df = pd.concat([df, new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text     topic\n",
      "0    Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n",
      "1    Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n",
      "2    Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n",
      "3    High fuel prices hit BA's profits\\n\\nBritish A...  business\n",
      "4    Pernod takeover talk lifts Domecq\\n\\nShares in...  business\n",
      "..                                                 ...       ...\n",
      "506  Big guns ease through in San Jose\\n\\nTop-seede...     sport\n",
      "507  Almagro continues Spanish surge\\n\\nUnseeded Ni...     sport\n",
      "508  Melzer shocks Agassi in San Jose\\n\\nSecond see...     sport\n",
      "509  Mirza makes Indian tennis history\\n\\nTeenager ...     sport\n",
      "510  Roddick to face Saulnier in final\\n\\nAndy Rodd...     sport\n",
      "\n",
      "[1824 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df.columns = [\"text\", \"topic\"]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[\"text\"]\n",
    "test = df[\"topic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#テキストの前処理\n",
    "#改行、大文字小文字の区別、ストップワードに関して\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def stopword(text):\n",
    "    new =  \" \".join([word for word in str(text).split() if word not in stopwords])\n",
    "    return new\n",
    "    \n",
    "train = train.str.lower()\n",
    "train = train.str.replace('[^\\w\\s]','')\n",
    "train = train.str.replace(\"\\n\", \"\")\n",
    "train.apply(stopword)\n",
    "train = train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(train, test, test_size=0.25, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 5\n",
    "vocab_size = 15000\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文章のベクトル化\n",
    "tokenizer = Tokenizer(num_words = vocab_size)\n",
    "tokenizer.fit_on_texts(train)\n",
    "\n",
    "trainx = tokenizer.texts_to_matrix(train_x, mode=\"tfidf\")\n",
    "testx = tokenizer.texts_to_matrix(test_x, mode=\"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1368, 15000)\n"
     ]
    }
   ],
   "source": [
    "print(trainx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#カテゴリ変数をエンコード\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(topics)\n",
    "train_y = encoder.transform(train_y)\n",
    "test_y = encoder.transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = trainx.reshape(-1, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               7680512   \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 2565      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 7,945,733\n",
      "Trainable params: 7,945,733\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "13/13 [==============================] - 4s 202ms/step - loss: 0.8714 - accuracy: 0.6648 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
      "Epoch 2/30\n",
      "13/13 [==============================] - 1s 96ms/step - loss: 0.0125 - accuracy: 0.9956 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 3/30\n",
      "13/13 [==============================] - 1s 100ms/step - loss: 3.1475e-05 - accuracy: 1.0000 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
      "Epoch 4/30\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 1.7386e-05 - accuracy: 1.0000 - val_loss: 0.0192 - val_accuracy: 0.9854\n",
      "Epoch 5/30\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 6.1257e-05 - accuracy: 1.0000 - val_loss: 0.0216 - val_accuracy: 0.9854\n",
      "Epoch 6/30\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 7.7699e-06 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9854\n",
      "Epoch 7/30\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 2.5531e-06 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9854\n",
      "Epoch 8/30\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 9.5658e-07 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9854\n",
      "Epoch 9/30\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 2.1245e-06 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9854\n",
      "Epoch 10/30\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 2.1112e-06 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 0.9854\n",
      "Epoch 11/30\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 1.0698e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9854\n",
      "Epoch 12/30\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 5.1752e-06 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9854\n",
      "Epoch 13/30\n",
      "13/13 [==============================] - 1s 99ms/step - loss: 1.0070e-06 - accuracy: 1.0000 - val_loss: 0.0217 - val_accuracy: 0.9854\n",
      "Epoch 14/30\n",
      "13/13 [==============================] - 1s 110ms/step - loss: 6.7212e-07 - accuracy: 1.0000 - val_loss: 0.0216 - val_accuracy: 0.9854\n",
      "Epoch 15/30\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 1.7436e-06 - accuracy: 1.0000 - val_loss: 0.0215 - val_accuracy: 0.9854\n",
      "Epoch 16/30\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 4.3674e-06 - accuracy: 1.0000 - val_loss: 0.0214 - val_accuracy: 0.9854\n",
      "Epoch 17/30\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 5.6347e-07 - accuracy: 1.0000 - val_loss: 0.0211 - val_accuracy: 0.9854\n",
      "Epoch 18/30\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 1.5566e-06 - accuracy: 1.0000 - val_loss: 0.0207 - val_accuracy: 0.9854\n",
      "Epoch 19/30\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 7.3385e-07 - accuracy: 1.0000 - val_loss: 0.0204 - val_accuracy: 0.9854\n",
      "Epoch 20/30\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 1.1737e-06 - accuracy: 1.0000 - val_loss: 0.0203 - val_accuracy: 0.9854\n",
      "Epoch 21/30\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 1.2900e-06 - accuracy: 1.0000 - val_loss: 0.0201 - val_accuracy: 0.9854\n",
      "Epoch 22/30\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 4.8853e-07 - accuracy: 1.0000 - val_loss: 0.0200 - val_accuracy: 0.9854\n",
      "Epoch 23/30\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 1.2531e-06 - accuracy: 1.0000 - val_loss: 0.0199 - val_accuracy: 0.9854\n",
      "Epoch 24/30\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 9.3710e-07 - accuracy: 1.0000 - val_loss: 0.0198 - val_accuracy: 0.9854\n",
      "Epoch 25/30\n",
      "13/13 [==============================] - 1s 97ms/step - loss: 2.0599e-06 - accuracy: 1.0000 - val_loss: 0.0202 - val_accuracy: 0.9854\n",
      "Epoch 26/30\n",
      "13/13 [==============================] - 1s 96ms/step - loss: 1.9438e-06 - accuracy: 1.0000 - val_loss: 0.0211 - val_accuracy: 0.9854\n",
      "Epoch 27/30\n",
      "13/13 [==============================] - 1s 100ms/step - loss: 7.0361e-07 - accuracy: 1.0000 - val_loss: 0.0213 - val_accuracy: 0.9854\n",
      "Epoch 28/30\n",
      "13/13 [==============================] - 1s 102ms/step - loss: 9.1080e-07 - accuracy: 1.0000 - val_loss: 0.0211 - val_accuracy: 0.9854\n",
      "Epoch 29/30\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 1.1546e-06 - accuracy: 1.0000 - val_loss: 0.0214 - val_accuracy: 0.9854\n",
      "Epoch 30/30\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 1.0203e-06 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9854\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = model.fit(trainx, train_y,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0739 - accuracy: 0.9759\n",
      "Test accuracy: 0.9758771657943726\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(testx, test_y,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    " \n",
    "#depp_learningモデルを使用した結果、97％の精度",
    "print('Test accuracy:', score[1])\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-6ecd015f6e4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'my_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizer.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "model.model.save('my_model.h5')\n",
    " \n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
