{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n",
      "['上先生と私一私はその人を常に先生と呼んでいた', 'だからここでもただ先生と書くだけで本名は打ち明けない', 'これは世間を憚かる遠慮というよりも、その方が私にとって自然だからである', '私はその人の記憶を呼び起すごとに、すぐ先生']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "url = 'https://www.aozora.gr.jp/cards/000148/files/773_14560.html' #青空文庫\n",
    "res = requests.get(url)\n",
    "try:\n",
    " res.raise_for_status()\n",
    "except Exception as exc:\n",
    " print('Error : {}'.format(exc))\n",
    "\n",
    "from bs4 import BeautifulSoup as bs4\n",
    "no_strach_soup = bs4(res.content, 'lxml')\n",
    "print(type(no_strach_soup))\n",
    "contents = no_strach_soup.find('div', class_=\"main_text\")\n",
    "text = contents.get_text()\n",
    "\n",
    "#テキストの前処理\n",
    "#不要な記号などを省く\n",
    "import re\n",
    "text = re.sub('（.+?）', '', text) \n",
    "text = re.sub('\\n\\n', '\\n', text)\n",
    "text = re.sub('\\r', '', text)\n",
    "text = re.sub(r'[\\u3000 \\t]','', text) \n",
    "text = re.sub(r'\\（.+\\）', '', text)\n",
    "text = text.replace(\"「\", \"\")\n",
    "text = text.replace(\"」\", \"。\")\n",
    "text = text.replace(\"\\n\", \"\")\n",
    "text = text.strip()\n",
    "\n",
    "text = text.split(\"。\")\n",
    "print(text[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "t = Tokenizer()\n",
    "sentences = []\n",
    "\n",
    "#形態素解析\n",
    "#ストップワードとして、助詞など意味の薄い単語は省いた\n",
    "for sentence in text:\n",
    "    words = []\n",
    "    for token in t.tokenize(sentence):\n",
    "        hinshi = token.part_of_speech.split(\",\")\n",
    "        if  hinshi[0] == \"名詞\" or hinshi[0] == \"動詞\" or hinshi[0] == \"形容詞\" or hinshi[0] == \"副詞\":\n",
    "            words.append(token.surface)\n",
    "    sentences.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['上', '先生', '私', '一', '私', '人', '常に', '先生', '呼ん', 'い'], ['ここ', '先生', '書く', '本名', '打ち明け'], ['これ', '世間', '憚', 'かる', '遠慮', 'いう', '方', '私', '自然'], ['私', '人', '記憶', '呼び', '起す', 'ごと', 'すぐ', '先生'], ['いい', 'なる'], ['筆', '執っ', '心持', '事'], ['よそよそしい', '頭文字', 'とても', '使う', '気', 'なら'], ['私', '先生', '知り合い', 'なっ', 'の', '鎌倉', 'ある'], ['時', '私', 'まだ', '若々しい', '書生'], ['暑中', '休暇', '利用', 'し', '海水浴', '行っ', '友達', 'ぜひ', '来い', '端書', '受け取っ', '私', '多少', '金', '工面', 'し', '出掛ける', '事', 'し']]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1 = sum(sentences, []) \n",
    "words1 = set(words1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordtoid = {}\n",
    "idtoword = {}\n",
    "for i, word in enumerate(words1):\n",
    "    idtoword[i] = word\n",
    "    wordtoid[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文章をベクトル化するための辞書:corpusを作る\n",
    "corpus = []\n",
    "for sentence in sentences:\n",
    "    sub = []\n",
    "    for word in sentence:\n",
    "        sub.append(wordtoid[word])\n",
    "    corpus.append(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6367, 4233, 4170, 386, 4170, 5663, 6034, 4233, 148, 4897], [5779, 4233, 6148, 1754, 3452], [5187, 5214, 3000, 5465, 4594, 85, 3716, 4170, 4313], [4170, 5663, 4145, 5069, 1152, 574, 4434, 4233], [843, 6093], [3577, 791, 2607, 2828], [5926, 5642, 993, 4174, 5420, 5956], [4170, 4233, 3220, 3314, 4241, 4193, 1953], [5637, 4170, 149, 1034, 1379], [4628, 1915, 2286, 3583, 5101, 4988, 3822, 4907, 2764, 2336, 956, 4170, 4731, 3227, 5740, 3583, 738, 2828, 3583]]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size*2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word   = []            \n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            context_words.append([words[i] \n",
    "                                 for i in range(start, end) \n",
    "                                 if 0 <= i < sentence_length \n",
    "                                 and i != index])\n",
    "            label_word.append(word)\n",
    "\n",
    "            x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
    "            y = np_utils.to_categorical(label_word, vocab_size)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (X): ['上', '先生', '一', '私'] -> Target (Y): 私\n",
      "Context (X): ['先生', '私', '私', '人'] -> Target (Y): 一\n",
      "Context (X): ['私', '一', '人', '常に'] -> Target (Y): 私\n",
      "Context (X): ['一', '私', '常に', '先生'] -> Target (Y): 人\n",
      "Context (X): ['私', '人', '先生', '呼ん'] -> Target (Y): 常に\n",
      "Context (X): ['人', '常に', '呼ん', 'い'] -> Target (Y): 先生\n",
      "Context (X): ['ここ', '先生', '本名', '打ち明け'] -> Target (Y): 書く\n",
      "Context (X): ['これ', '世間', 'かる', '遠慮'] -> Target (Y): 憚\n",
      "Context (X): ['世間', '憚', '遠慮', 'いう'] -> Target (Y): かる\n",
      "Context (X): ['憚', 'かる', 'いう', '方'] -> Target (Y): 遠慮\n",
      "Context (X): ['かる', '遠慮', '方', '私'] -> Target (Y): いう\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(wordtoid)\n",
    "embed_size = 100\n",
    "window_size = 2 \n",
    "\n",
    "#ウィンドウ（文脈の広さ）を２に設定\n",
    "#共起行列を作る\n",
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=corpus, window_size=window_size, vocab_size=vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [idtoword[w] for w in x[0]], '-> Target (Y):', idtoword[np.argwhere(y[0])[0][0]])\n",
    "    \n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 100)            640300    \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6403)              646703    \n",
      "=================================================================\n",
      "Total params: 1,287,003\n",
      "Trainable params: 1,287,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'create'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-5a57e0409d76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvis_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mSVG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcbow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'TB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dot'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'svg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'create'"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "\n",
    "# build CBOW architecture\n",
    "cbow = Sequential()\n",
    "#embedding layer > onehot into vectorizering\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# view model summary\n",
    "print(cbow.summary())\n",
    "\n",
    "# visualize model structure\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False, rankdir='TB').create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 596023.1958738164\n",
      "\n",
      "Epoch: 2 \tLoss: 604676.7702609171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 3):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for x, y in generate_context_word_pairs(corpus=corpus, window_size=window_size, vocab_size=vocab_size):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        if i % 100000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6402, 6402)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'先生': ['ため', 'ある', '行っ', '前', '言葉', 'すぐ', '一', 'なっ'],\n",
       " '私': ['し', 'の', 'もの', 'よう', '上', 'い', 'する', '中'],\n",
       " '東京': ['学校', '好い', '得', '後', '下', '身体', '場合', '我々'],\n",
       " '故郷': ['程度', '狭い', 'がら', '入り', '試み', '来い', '専門', '立派']}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = cbow.get_weights()[0]\n",
    "weights = weights[1:]\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "print(distance_matrix.shape)\n",
    "\n",
    "similar_words = {search_term: [idtoword[idx] for idx in distance_matrix[wordtoid[search_term]-1].argsort()[1:9]+1] \n",
    "                   for search_term in [\"先生\", \"私\", \"東京\", \"故郷\"]}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
